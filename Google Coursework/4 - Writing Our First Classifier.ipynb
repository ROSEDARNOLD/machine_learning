{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline from pt.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cjrdn_000\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomClassification():\n",
    "    \n",
    "    # we know that classifiers require fit (training) and predict (prediction) methods generally:\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test, y_test):\n",
    "        \n",
    "        #recall that X_train is a nested list of features\n",
    "        \n",
    "        predictions = []\n",
    "        for row in X_test: #for every feature set\n",
    "            label = random.choice(self.y_train) #draw a label from the iris training targets at randomn\n",
    "            predictions.append(label) # append that label to a list of predictions\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note that our Random classifier has an ~ 33% accuracy, which comports with our knowledge of the dataset (3 labels, each represented equally in our dataset, which is randomnly partitioned into training and testing sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def euc(a, b):\n",
    "    \n",
    "    return distance.euclidean(a, b)\n",
    "\n",
    "class ScrappyKNN():\n",
    "    \n",
    "    # we know that classifiers require fit (training) and predict (prediction) methods generally:\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test, y_test):\n",
    "        \n",
    "        #pick the label of the found most commonly on the nearest k (1) neighbors; proximity measured by euclidian distance\n",
    "        \n",
    "        predictions = []\n",
    "        for row in X_test: #for every feature set\n",
    "            label = self.closest(row) #draw a label from the iris training targets at randomn\n",
    "            predictions.append(label) # append that label to a list of predictions\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def closest(self, row):\n",
    "        \n",
    "        #initialize a null value from which to consider every point in our training data (distance to the first training obs)\n",
    "        best_dist = euc(row, self.X_train[0])\n",
    "        best_index = 0\n",
    "        \n",
    "        for i in range(1, len(self.X_train)):\n",
    "            if euc(row, X_train[i]) < best_dist:\n",
    "                best_dist = euc(row, X_train[i])\n",
    "                best_index = i\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        label = y_train[best_index]\n",
    "            \n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973333333333\n"
     ]
    }
   ],
   "source": [
    "#my_classifier = RandomClassification()\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "\n",
    "my_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = my_classifier.predict(X_test, y_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros of simple KNN: Easy to understand\n",
    "\n",
    "# Cons: Computationally expensive, \n",
    "\n",
    "# Hard to represent relationships between features:\n",
    "#           Recall: 1 feature set is [sepal length, sepal width, petal length, petal width]\n",
    "# If sepal length were more important in the determination of iris type than petal length, how would we represent that using our setup of euclidian distance?\n",
    "\n",
    "# We can use other classifiers (decision trees, nueral nets) to better represent the more complex relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
