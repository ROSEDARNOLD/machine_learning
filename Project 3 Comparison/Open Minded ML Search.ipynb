{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Selector():\n",
    "        \n",
    "    '''Non-operational bag of Methods for selecting between model types and feature inputs given a model.  Parent of the\n",
    "    operant classes GenericClassifier and GenericRegressor'''    \n",
    "    \n",
    "    def ModelSelection(self, folds=10, rstate=420):\n",
    "        \n",
    "        cv_scores, cv_summary = {}, {}\n",
    "        \n",
    "        \n",
    "        for name, model in self.Models.items():\n",
    "            \n",
    "            try:\n",
    "            \n",
    "                kfold = model_selection.KFold(n_splits=folds, random_state=rstate) \n",
    "                cv_result = model_selection.cross_val_score(model, self.X, self.y, cv=kfold, scoring='accuracy')\n",
    "                cv_summary = \"%s: %f (%f)\" % (name, cv_result.mean(), cv_result.std())\n",
    "                cv_scores[name] = cv_result       \n",
    "                \n",
    "            \n",
    "            except Exception as e:\n",
    "                \n",
    "                cv_scores[name] = e\n",
    "                cv_summary[name] = e\n",
    "        \n",
    "        self.cv_scores = cv_scores\n",
    "        \n",
    "        # Print Summary\n",
    "        for k, v in Model.cv_scores.items():\n",
    "    \n",
    "           msg = \"%s: %f (%f)\" % (k, v.mean(), v.std())\n",
    "           print(msg)\n",
    "            \n",
    "        # We could return a 'best model' for ease of use, but it will require us to be explicit about our selection criteria\n",
    "        # (MSE, std errors, priors) up front -> seems exceptionally black boxy; we should probably just look at the results\n",
    "        # and decide manually (What else would anyone pay us for?).\n",
    "\n",
    "        \n",
    "    def FeatureSelection(self, folds=10, rstate=420):\n",
    "        \n",
    "        '''This section is considerably more sketchy than the model selection component; needs work\n",
    "        before results are to be trusted'''\n",
    "        \n",
    "        feature_cols = self.X.columns\n",
    "        scores = {}\n",
    "        kfold = model_selection.KFold(n_splits=folds, random_state=rstate)\n",
    "        model = self.best_model\n",
    "        model.fit(self.X, self.y)\n",
    "        mse_scores = -model_selection.cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "        scores[None] = mse_scores\n",
    "        \n",
    "        for dropped_x in feature_cols:\n",
    "    \n",
    "            feature_subset = [item for item in feature_cols if item != dropped_x]\n",
    "            X2 = self.X[feature_subset]\n",
    "            model = self.best_model\n",
    "            model.fit(X2, y)\n",
    "            mse_scores = -model_selection.cross_val_score(model, X2, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "            scores[dropped_x] = mse_scores\n",
    "        \n",
    "        self.feature_scores = scores\n",
    "        \n",
    "        summary = {key: {'MEAN MSE': value.mean(), 'MEAN RMSE': np.sqrt(value).mean()} for key, value in scores.items()}\n",
    "        self.feature_summary = summary \n",
    "        \n",
    "class GenericClassifier(Selector):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.Models = {\n",
    "                       \n",
    "            'LR': LogisticRegression(),\n",
    "            'KNN': KNeighborsClassifier(),\n",
    "            'GBT': GradientBoostingClassifier(),\n",
    "            'NB': GaussianNB(),\n",
    "            'SVM': SVC(),\n",
    "            'DT': DecisionTreeClassifier()\n",
    "        \n",
    "        }\n",
    "        \n",
    "class GenericRegressor(Selector):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.Models = {\n",
    "                       \n",
    "            # 'OLS': LinearRegression(),\n",
    "            # etc..\n",
    "        \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cable_training = pd.read_csv('./Data/training.csv', na_values=(-999, 6)) # value = 6 corresponds to refusal to answer, 6 nowhere else in data\n",
    "cable_holdout = pd.read_csv('./Data/holdout.csv', na_values=(-999, 6))\n",
    "cable = pd.concat([cable_training, cable_holdout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CleanCableData(df):\n",
    "    \n",
    "    # Since this is a purely exploratory excercise in ML, we have no priors about inappropriate information, beyond the obvious (ID)\n",
    "    \n",
    "    #drop = ['YES', 'ID', 'age', 'class', 'tele_have']\n",
    "    #df['value'] = [(i - 3) for i in df['value']] # Normalize (-2 to +2)\n",
    "    #df = df[[col for col in df.columns if col not in drop]]\n",
    "    \n",
    "    drop = ['ID', 'tele_have']\n",
    "    df = df[[col for col in df.columns if col not in drop]]\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cable = CleanCableData(cable)\n",
    "\n",
    "cable['constant'] = [1 for i in range(len(cable))]\n",
    "\n",
    "y = pd.DataFrame(cable['buy'])\n",
    "X = cable[[col for col in cable.columns if col != 'buy']]\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model = GenericClassifier(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.871505 (0.006737)\n",
      "GBT: 0.849268 (0.005532)\n",
      "LR: 0.763249 (0.015521)\n",
      "DT: 0.998402 (0.001082)\n",
      "NB: 0.743675 (0.017478)\n",
      "KNN: 0.998535 (0.000883)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Model.ModelSelection(folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are these cv_scores telling me?\n",
    "\n",
    "# Per documentation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "# Returns: Array of scores of the estimators prediction for each run of the cross validation.\n",
    "# So a cv score of '1' in a classification problem means perfect prediction over the holdout? -> ok, seems fine\n",
    "\n",
    "# STD of cv scores gives an indication of how sensitive my MSE (generalization error) is w.r.t. my sampling procedure.\n",
    "# Do I pick a classifier based on lowest mean error (holdout accuracy) or lowest variance of that error?\n",
    "\n",
    "# Highest Reported Accuracy -> KNN\n",
    "# Lowest Variance -> Still KNN\n",
    "\n",
    "# An MSE and standard error of ~zero seems peculiar; its' difficult to believe that the model can predict consumer\n",
    "# behavior with almost 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is KNN doing?  \n",
    "# Retrain 'best model' on the entire sample\n",
    "\n",
    "Model = KNeighborsClassifier()\n",
    "Model.fit(X, y)\n",
    "\n",
    "#...\n",
    "\n",
    "# Double checked the model to make sure predictions were in the correct dimensions, etc. No glaring problems\n",
    "# detected.  Still, I find this result very odd.  Use LIME as a tie-breaker?\n",
    "\n",
    "# For a discussion on hyperparameter selection, please see the 'Proof of Concept' notebook for automatic\n",
    "# Model selection; we're a bit fuzzy on this and could use an in person discussion to clean some stuff up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have an opinion on the believability of our 'Applied' like model (see: ), but there's no equivalent\n",
    "# testing framework to evaulate the claims of the KNN model -> Use Linear Approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
